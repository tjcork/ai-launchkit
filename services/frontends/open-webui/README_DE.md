# üí¨ Open WebUI - ChatGPT-Oberfl√§che

### Was ist Open WebUI?

Open WebUI ist eine selbst gehostete ChatGPT-√§hnliche Oberfl√§che, die eine sch√∂ne, funktionsreiche Chat-Erfahrung f√ºr lokale und Remote-LLMs bietet. Sie integriert sich nahtlos mit Ollama f√ºr lokale Modelle, OpenAI API und andere Anbieter und bietet Konversationsverwaltung, Modellwechsel und erweiterte Funktionen wie RAG (Retrieval-Augmented Generation).

### Funktionen

- **ChatGPT-√§hnliche Oberfl√§che** - Vertraute Chat-UI mit Markdown, Code-Highlighting und Streaming
- **Mehrere Modelle unterst√ºtzt** - Wechsle zwischen Ollama (lokal), OpenAI, Anthropic und anderen Anbietern
- **Konversationsverwaltung** - Speichere, durchsuche und organisiere Chat-Verl√§ufe
- **RAG-Integration** - Lade Dokumente hoch und chatte mit deinen Daten √ºber Vektorsuche
- **Multi-User-Unterst√ºtzung** - Benutzerkonten, Authentifizierung und rollenbasierter Zugriff
- **Modellbibliothek** - Durchsuche und lade Ollama-Modelle direkt aus der UI herunter

### Erste Einrichtung

**Erste Anmeldung bei Open WebUI:**

1. Navigiere zu `https://webui.deinedomain.com`
2. **Erster Benutzer wird Admin** - Erstelle dein Konto
3. Setze ein starkes Passwort
4. Einrichtung abgeschlossen!

**Ollama ist vorkonfiguriert** - Alle lokalen Modelle von Ollama sind automatisch verf√ºgbar.

### Verbindung zu Ollama-Modellen

**Ollama ist bereits intern verbunden:**

- **Interne URL:** `http://ollama:11434`
- Alle Modelle, die in Ollama geladen sind, erscheinen automatisch in Open WebUI
- Keine zus√§tzliche Konfiguration erforderlich!

**Verf√ºgbare Standardmodelle:**
- `llama3.2` - Schnell, universell einsetzbar (empfohlen)
- `mistral` - Hervorragend f√ºr Coding und Reasoning
- `llama3.2-vision` - Multimodal (Text + Bilder)
- `qwen2.5-coder` - Spezialisiert f√ºr Code-Generierung

### Zus√§tzliche Modelle herunterladen

**Option 1: √úber Open WebUI (empfohlen)**

1. Klicke auf Einstellungen (Zahnrad-Symbol)
2. Gehe zum Tab **Modelle**
3. Durchsuche verf√ºgbare Modelle
4. Klicke auf **Pull** zum Herunterladen

**Option 2: √úber die Kommandozeile**

```bash
# Ein bestimmtes Modell herunterladen
docker exec ollama ollama pull llama3.2

# Installierte Modelle auflisten
docker exec ollama ollama list

# Ein Modell entfernen
docker exec ollama ollama rm modellname
```

**Beliebte Modell-Empfehlungen:**

| Modell | Gr√∂√üe | Am besten f√ºr | RAM erforderlich |
|-------|------|----------|--------------|
| `llama3.2` | 2GB | Allgemeiner Chat, schnell | 4GB |
| `llama3.2:70b` | 40GB | Beste Qualit√§t | 64GB+ |
| `mistral` | 4GB | Coding, Reasoning | 8GB |
| `qwen2.5-coder:7b` | 4GB | Code-Generierung | 8GB |
| `llama3.2-vision` | 5GB | Bildverst√§ndnis | 8GB |
| `deepseek-r1:7b` | 4GB | Reasoning, Mathematik | 8GB |

### OpenAI API-Modelle hinzuf√ºgen

**Verbinde dich mit OpenAI f√ºr schnellere Antworten:**

1. Einstellungen ‚Üí **Verbindungen**
2. Abschnitt **OpenAI API**
3. API-Schl√ºssel hinzuf√ºgen: `sk-your-key-here`
4. OpenAI-Modelle aktivieren

**Verf√ºgbare Modelle:**
- `gpt-4o` - Am leistungsf√§higsten (empfohlen)
- `gpt-4o-mini` - Schnell und kosteneffektiv
- `o1` - Fortgeschrittenes Reasoning

### RAG (Chatte mit deinen Dokumenten)

**Dokumente hochladen und damit chatten:**

1. Klicke auf das **+** Symbol im Chat
2. W√§hle **Dateien hochladen**
3. W√§hle PDF, DOCX, TXT oder andere Dokumente
4. Open WebUI erledigt automatisch:
   - Textextraktion
   - Erstellt Embeddings
   - Speichert in Vektordatenbank
5. Stelle Fragen zu deinen Dokumenten!

**Beispiel-Anfragen:**
```
"Fasse die wichtigsten Erkenntnisse aus dem hochgeladenen Bericht zusammen"
"Was sagt der Vertrag √ºber die K√ºndigung?"
"Extrahiere alle Aktionspunkte aus den Meeting-Notizen"
```

### n8n-Integration

**HTTP-Request an Open WebUI API:**

```javascript
// HTTP Request Node Konfiguration
Methode: POST
URL: http://open-webui:8080/api/chat/completions
Authentication: Bearer Token
  Token: [Dein Open WebUI API Key]
  
Body (JSON):
{
  "model": "llama3.2",
  "messages": [
    {
      "role": "system",
      "content": "Du bist ein hilfreicher Assistent"
    },
    {
      "role": "user",
      "content": "{{$json.user_question}}"
    }
  ],
  "stream": false
}

// Antwort:
{
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "KI-Antwort hier..."
      }
    }
  ]
}
```

**API-Schl√ºssel in Open WebUI generieren:**
1. Einstellungen ‚Üí **Konto** ‚Üí **API-Schl√ºssel**
2. Klicke auf **Neuen API-Schl√ºssel erstellen**
3. Kopieren und sicher speichern

### Beispiel-Workflows

#### Beispiel 1: Kundensupport-Automatisierung

```javascript
// 1. Webhook Trigger - Support-Ticket empfangen
// Input: { "customer": "John", "question": "Wie setze ich mein Passwort zur√ºck?" }

// 2. HTTP Request - Open WebUI abfragen
Methode: POST
URL: http://open-webui:8080/api/chat/completions
Header:
  Authorization: Bearer {{$env.OPENWEBUI_API_KEY}}
Body: {
  "model": "llama3.2",
  "messages": [
    {
      "role": "system",
      "content": "Du bist ein Kundensupport-Assistent. Gib klare, hilfreiche Antworten basierend auf unserer Wissensdatenbank."
    },
    {
      "role": "user",
      "content": "{{$json.question}}"
    }
  ]
}

// 3. Code Node - Antwort formatieren
const response = $json.choices[0].message.content;
return {
  customer: $('Webhook').item.json.customer,
  question: $('Webhook').item.json.question,
  ai_response: response,
  timestamp: new Date().toISOString()
};

// 4. E-Mail senden - Kunde antworten
To: {{$json.customer}}@company.com
Subject: Re: {{$json.question}}
Nachricht: |
  Hallo {{$json.customer}},
  
  {{$json.ai_response}}
  
  Mit freundlichen Gr√º√üen,
  Support-Team

// 5. Baserow Node - Interaktion protokollieren
Table: support_tickets
Fields: {
  customer: {{$json.customer}},
  question: {{$json.question}},
  ai_response: {{$json.ai_response}},
  resolved: true
}
```

#### Beispiel 2: Dokumenten-Analyse-Pipeline

```javascript
// 1. Schedule Trigger - T√§glich um 9 Uhr
// 2. Google Drive - Neue PDFs abrufen
Folder: /Documents/ToProcess
Dateityp: PDF

// 3. HTTP Request - An Open WebUI mit RAG hochladen
Methode: POST
URL: http://open-webui:8080/api/documents/upload
Header:
  Authorization: Bearer {{$env.OPENWEBUI_API_KEY}}
Body (Form Data):
  Datei: {{$binary.data}}

// 4. HTTP Request - Dokument abfragen
Methode: POST
URL: http://open-webui:8080/api/chat/completions
Body: {
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "Analysiere dieses Dokument und extrahiere: 1) Hauptthemen, 2) Aktionspunkte, 3) Fristen"
    }
  ],
  "files": ["{{$json.document_id}}"]
}

// 5. Code Node - Ergebnisse strukturieren
const analysis = JSON.parse($json.choices[0].message.content);
return {
  document: $('Google Drive').item.json.name,
  key_topics: analysis.key_topics,
  action_items: analysis.action_items,
  deadlines: analysis.deadlines
};

// 6. Notion - Seite mit Analyse erstellen
Database: Project Docs
Properties: {
  title: {{$json.document}},
  topics: {{$json.key_topics}},
  actions: {{$json.action_items}},
  due_dates: {{$json.deadlines}}
}
```

#### Beispiel 3: Multi-Modell-Vergleich

```javascript
// Vergleiche Antworten von verschiedenen Modellen

// 1. Webhook Trigger - Frage empfangen

// 2. Split in Batches (parallele Ausf√ºhrung)

// 3a. HTTP Request - Ollama (Llama)
URL: http://open-webui:8080/api/chat/completions
Body: {
  "model": "llama3.2",
  "messages": [{"role": "user", "content": "{{$json.question}}"}]
}

// 3b. HTTP Request - OpenAI (GPT-4)
URL: http://open-webui:8080/api/chat/completions
Body: {
  "model": "gpt-4o",
  "messages": [{"role": "user", "content": "{{$json.question}}"}]
}

// 3c. HTTP Request - Mistral
URL: http://open-webui:8080/api/chat/completions
Body: {
  "model": "mistral",
  "messages": [{"role": "user", "content": "{{$json.question}}"}]
}

// 4. Aggregate Results
// Alle Antworten kombinieren

// 5. Code Node - Vergleichen und bewerten
const responses = [
  { model: "llama3.2", answer: $item(0).json.choices[0].message.content },
  { model: "gpt-4o", answer: $item(1).json.choices[0].message.content },
  { model: "mistral", answer: $item(2).json.choices[0].message.content }
];

// Beste Antwort basierend auf L√§nge, Klarheit, etc. zur√ºckgeben
return responses;
```

### LightRAG-Integration

**LightRAG als Modell in Open WebUI hinzuf√ºgen:**

1. Einstellungen ‚Üí **Verbindungen**
2. Neue Ollama-Verbindung hinzuf√ºgen:
   - **URL:** `http://lightrag:9621`
   - **Modellname:** `lightrag:latest`
3. LightRAG aus dem Modell-Dropdown ausw√§hlen

**Jetzt kannst du direkt mit deinem Wissensgraph chatten!**

### Benutzerverwaltung

**Admin-Funktionen:**

1. Einstellungen ‚Üí **Admin-Panel**
2. Benutzer, Rollen, Berechtigungen verwalten
3. Modellzugriff pro Benutzer steuern
4. Nutzungsstatistiken anzeigen

**Zus√§tzliche Benutzer erstellen:**

1. Admin-Panel ‚Üí **Benutzer** ‚Üí **Benutzer hinzuf√ºgen**
2. Benutzername, E-Mail, Passwort festlegen
3. Rolle zuweisen: Admin, Benutzer oder Ausstehend
4. Benutzer k√∂nnen sich auch selbst registrieren, falls aktiviert

### Fehlerbehebung

**Modelle werden nicht angezeigt:**

```bash
# Pr√ºfe ob Ollama l√§uft
docker ps | grep ollama

# Verbindung von Open WebUI √ºberpr√ºfen
docker exec open-webui curl http://ollama:11434/api/tags

# Open WebUI neu starten
docker compose restart open-webui
```

**Langsame Antworten:**

```bash
# Serverressourcen √ºberpr√ºfen
docker stats ollama

# Modell k√∂nnte zu gro√ü f√ºr RAM sein
# Zu kleinerem Modell wechseln:
# llama3.2 (2GB) statt llama3.2:70b (40GB)

# CPU-Zuweisung in docker-compose.yml erh√∂hen
```

**RAG funktioniert nicht:**

```bash
# Vektordatenbank √ºberpr√ºfen
docker logs open-webui | grep vector

# Embeddings l√∂schen und neu aufbauen
# Einstellungen ‚Üí Admin ‚Üí Vektordatenbank zur√ºcksetzen

# Ausreichend Speicherplatz sicherstellen
df -h
```

**API-Authentifizierung fehlgeschlagen:**

```bash
# API-Schl√ºssel in Open WebUI neu generieren
# Einstellungen ‚Üí Konto ‚Üí API-Schl√ºssel ‚Üí Neu erstellen

# n8n-Credential aktualisieren
# Alten Schl√ºssel durch neuen ersetzen
```

### Ressourcen

- **Offizielle Dokumentation:** https://docs.openwebui.com/
- **GitHub:** https://github.com/open-webui/open-webui
- **API-Referenz:** https://docs.openwebui.com/api/
- **Modellbibliothek:** https://ollama.com/library
- **Community Discord:** https://discord.gg/open-webui

### Best Practices

**Modellauswahl:**
- Verwende `llama3.2` f√ºr allgemeine Aufgaben (schnell, 2GB)
- Verwende `gpt-4o-mini` f√ºr bessere Qualit√§t, wenn Geschwindigkeit wichtig ist
- Verwende `qwen2.5-coder` f√ºr code-lastige Aufgaben
- Verwende Vision-Modelle f√ºr Bildanalyse

**Performance:**
- Behalte 2-3 h√§ufig verwendete Modelle heruntergeladen
- Entferne ungenutzte Modelle um Speicherplatz zu sparen
- Verwende OpenAI API f√ºr Produktion (schneller, zuverl√§ssiger)
- Verwende Ollama f√ºr datenschutzsensible Daten

**Sicherheit:**
- √Ñndere das Standard-Admin-Passwort sofort
- Deaktiviere Selbstregistrierung in Produktion
- Verwende rollenbasierten Zugriff f√ºr Team-Deployments
- Sichere regelm√§√üig die Konversationsverl√§ufe

**RAG-Optimierung:**
- Lade Dokumente in unterst√ºtzten Formaten hoch (PDF, DOCX, TXT)
- Halte Dokumente unter 10MB f√ºr beste Performance
- Verwende klare, beschreibende Fragen
- Kombiniere mehrere verwandte Dokumente f√ºr besseren Kontext
