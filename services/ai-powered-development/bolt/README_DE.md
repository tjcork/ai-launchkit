# ‚ö° bolt.diy - KI-App-Builder

### Was ist bolt.diy?

bolt.diy ist eine KI-gest√ºtzte Full-Stack-Entwicklungsplattform, die es dir erm√∂glicht, vollst√§ndige Webanwendungen mithilfe nat√ºrlicher Sprachprompts zu erstellen. Basierend auf StackBlitz's bolt-Technologie kombiniert es KI-Unterst√ºtzung mit einer Live-Entwicklungsumgebung und erm√∂glicht schnelles Prototyping und MVP-Erstellung ohne tiefgehende Programmierkenntnisse.

### Funktionen

- **KI-gest√ºtzte Entwicklung**: Beschreibe deine App in nat√ºrlicher Sprache, beobachte wie sie in Echtzeit gebaut wird
- **Full-Stack-Unterst√ºtzung**: Frontend (React, Vue, Svelte) und Backend (Node.js, Python) in einer Umgebung
- **Live-Vorschau**: Sieh √Ñnderungen sofort mit Hot Module Replacement
- **Code-Export**: Lade vollst√§ndige Projekte mit allen Abh√§ngigkeiten herunter
- **Multi-Modell-Unterst√ºtzung**: Funktioniert mit OpenAI, Anthropic, Groq und anderen LLM-Anbietern
- **WebContainer-Technologie**: F√ºhrt Node.js direkt im Browser aus f√ºr sofortiges Feedback

### Ersteinrichtung

**Erster Zugriff auf bolt.diy:**
1. Navigiere zu `https://bolt.deinedomain.com`
2. Kein Login erforderlich - bolt.diy startet sofort
3. Konfiguriere API-Keys f√ºr dein bevorzugtes KI-Modell:
   - Klicke auf das Einstellungen-Symbol (‚öôÔ∏è) oben rechts
   - F√ºge deinen API-Key hinzu (OpenAI, Anthropic, Groq, etc.)
   - W√§hle dein bevorzugtes Modell (Claude Sonnet 3.5, GPT-4, etc.)

**Empfohlene Modelle:**
- **Claude 3.5 Sonnet**: Am besten f√ºr komplexe Full-Stack-Anwendungen
- **GPT-4**: Exzellent f√ºr React und Frontend-Entwicklung
- **Groq (Llama)**: Schnell, gut f√ºr schnelle Prototypen
- **Ollama**: Lokale Modelle, ben√∂tigt aktivierten Ollama-Service

### n8n-Integration einrichten

Obwohl bolt.diy keinen direkten n8n-Node hat, kannst du generierte Apps mit n8n-Workflows integrieren:

**Workflow-Muster: KI-App-Generierungs-Pipeline**

```javascript
// 1. Manual Trigger oder Webhook
// Benutzer √ºbermittelt App-Anforderungen

// 2. Code Node: bolt.diy-Prompt vorbereiten
const appSpec = {
  description: $json.userRequest,
  features: $json.requiredFeatures,
  tech_stack: "React + Node.js + PostgreSQL"
};

const boltPrompt = `Erstelle eine ${appSpec.tech_stack}-Anwendung:
${appSpec.description}

Erforderliche Funktionen:
${appSpec.features.join('\n')}

Inkludiere Authentifizierung, Datenbankmodelle und REST-API.`;

return { prompt: boltPrompt };

// 3. Manueller Schritt: Entwickler nutzt bolt.diy
// ‚Üí √ñffne bolt.deinedomain.com
// ‚Üí F√ºge den generierten Prompt ein
// ‚Üí √úberpr√ºfe und iteriere mit KI
// ‚Üí Exportiere den generierten Code

// 4. GitHub Node: Repository erstellen
// Exportierten Code zu GitHub hochladen

// 5. Webhook: Deployment-Pipeline ausl√∂sen
// ‚Üí Vercel/Netlify f√ºr Frontend
// ‚Üí Railway/Fly.io f√ºr Backend
```

**Interne URL:** `http://bolt:5173` (f√ºr interne Service-zu-Service-Kommunikation)

### Beispiel-Anwendungsf√§lle

#### Beispiel 1: Schnelle MVP-Entwicklung

**Szenario**: SaaS-Landingpage mit Authentifizierung in 10 Minuten erstellen

```
Prompt: "Erstelle eine moderne SaaS-Landingpage f√ºr einen KI-Schreibassistenten namens 'WriteWise'. 
Inkludiere:
- Hero-Sektion mit Gradient-Hintergrund
- Features-Sektion (3 Hauptfunktionen)
- Preistabelle (Free, Pro, Enterprise)
- E-Mail-Anmeldeformular mit Supabase-Integration
- Responsives Design mit Tailwind CSS"

Ergebnis: Vollst√§ndige React-App mit:
- Modernen UI-Komponenten
- Funktionierender Formular-Validierung
- Supabase-Auth-Integration
- Mobile-responsivem Layout
- Bereit zum Deployment
```

#### Beispiel 2: Interne Tool-Erstellung

**Szenario**: Benutzerdefiniertes Admin-Dashboard f√ºr dein Team erstellen

```
Prompt: "Erstelle ein Admin-Dashboard zur Verwaltung von AI CoreKit Services:
- Service-Status-√úbersicht (l√§uft/gestoppt)
- Ressourcennutzungs-Charts (CPU, RAM, Disk)
- Schnellaktionen (Services neu starten, Logs ansehen)
- Authentifizierung mit Benutzername/Passwort
- Dark-Mode-Unterst√ºtzung
- Nutze Express.js-Backend, React-Frontend"

Ergebnis: Full-Stack-Admin-Tool, das du:
- Intern deployen kannst
- Mit Docker-API verbinden kannst
- Mit zus√§tzlichen Prompts anpassen kannst
- Exportieren und selbst hosten kannst
```

#### Beispiel 3: API-Wrapper-Entwicklung

**Szenario**: Benutzerdefinierten API-Client f√ºr deine KI-Services erstellen

```
Prompt: "Baue einen Node.js-API-Wrapper f√ºr Ollama mit:
- TypeScript-Unterst√ºtzung
- Streaming-Antworten
- Konversationshistorien-Verwaltung
- Rate Limiting
- Fehlerbehandlung mit Retries
- Express-Server mit REST-Endpunkten"

Ergebnis: Produktionsreifer API-Wrapper, den du:
- In n8n-Workflows nutzen kannst
- Als Microservice deployen kannst
- Mit benutzerdefinierter Logik erweitern kannst
```

### Entwicklungs-Workflow

**Iterative Entwicklung mit bolt.diy:**

1. **Initialer Prompt**: Beginne mit einer klaren, detaillierten Beschreibung
2. **Generierten Code √ºberpr√ºfen**: Pr√ºfe Struktur und Abh√§ngigkeiten
3. **Mit Follow-ups verfeinern**: 
   - "F√ºge Benutzer-Authentifizierung hinzu"
   - "Mache es mobile-responsiv"
   - "F√ºge Fehlerbehandlung zu den API-Aufrufen hinzu"
4. **In Live-Vorschau testen**: Interagiere mit der App in Echtzeit
5. **Code exportieren**: Lade vollst√§ndiges Projekt mit package.json herunter
6. **Deployen**: Zu GitHub pushen, auf Hosting-Plattform deployen

**Best Practices:**
- **Sei spezifisch**: Detaillierte Prompts produzieren bessere Ergebnisse
- **Schrittweise iterieren**: F√ºge Funktionen einzeln hinzu
- **H√§ufig testen**: Nutze die Live-Vorschau, um Probleme fr√ºh zu erkennen
- **Oft exportieren**: Speichere Fortschritt vor gr√∂√üeren √Ñnderungen
- **Gute Modelle verwenden**: Claude 3.5 Sonnet oder GPT-4 f√ºr komplexe Apps

### Fehlerbehebung

**"Blocked Request" oder App l√§dt nicht:**

bolt.diy verwendet Vite, das Probleme mit Reverse Proxies haben kann. Dieser Fork enth√§lt automatische Hostname-Konfiguration.

```bash
# 1. Pr√ºfen, ob BOLT_HOSTNAME korrekt in .env gesetzt ist
grep BOLT_HOSTNAME .env
# Sollte zeigen: BOLT_HOSTNAME=bolt.deinedomain.com

# 2. Pr√ºfen, ob bolt.diy l√§uft
docker ps | grep bolt

# 3. bolt.diy-Logs auf Fehler pr√ºfen
docker logs bolt -f

# 4. Service neu starten
docker compose restart bolt

# 5. Browser-Cache leeren und erneut versuchen
# Chrome: Strg+Umschalt+Entf ‚Üí Zwischengespeicherte Bilder und Dateien l√∂schen
```

**KI-Modell antwortet nicht:**

```bash
# 1. API-Key in bolt.diy-Einstellungen verifizieren
# Auf Einstellungen-Symbol klicken ‚Üí API-Key-Format pr√ºfen

# 2. API-Key separat testen
curl https://api.anthropic.com/v1/messages \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "content-type: application/json" \
  -d '{"model":"claude-3-5-sonnet-20241022","max_tokens":10,"messages":[{"role":"user","content":"Hallo"}]}'

# 3. Rate Limits im Dashboard deines KI-Anbieters pr√ºfen
# OpenAI: platform.openai.com/usage
# Anthropic: console.anthropic.com
```

**Generierter Code funktioniert nicht:**

```bash
# 1. Browser-Konsole auf Fehler pr√ºfen (F12)
# Nach Dependency- oder Syntax-Fehlern suchen

# 2. package.json √ºberpr√ºfen
# Sicherstellen, dass alle Abh√§ngigkeiten kompatibel sind

# 3. Zuerst einen einfacheren Prompt versuchen
# Komplexit√§t schrittweise aufbauen

# 4. Besseres KI-Modell verwenden
# Von Groq zu Claude 3.5 Sonnet wechseln

# 5. Exportieren und lokal testen
npm install
npm run dev
```

**Langsame Generierungsgeschwindigkeit:**

```bash
# 1. Groq f√ºr schnellere Antworten verwenden (Trade-off: Qualit√§t)
# Einstellungen ‚Üí Groq ausw√§hlen ‚Üí llama-3.1-70b w√§hlen

# 2. Gro√üe Anfragen in kleinere Prompts aufteilen
# Statt: "Baue gesamte App"
# Nutze: "Baue Homepage" ‚Üí "F√ºge API hinzu" ‚Üí "F√ºge Auth hinzu"

# 3. Netzwerkverbindung pr√ºfen
# bolt.diy streamt Antworten in Echtzeit

# 4. Ressourcennutzung √ºberwachen
docker stats bolt
# Hohe CPU? Der Browser k√∂nnte mit gro√üen Projekten k√§mpfen
```

**Kann Code nicht exportieren oder herunterladen:**

```bash
# 1. Browser-Download-Einstellungen pr√ºfen
# Sicherstellen, dass Downloads nicht blockiert sind

# 2. Anderen Browser versuchen
# Firefox, Chrome, Safari funktionieren alle unterschiedlich

# 3. Code manuell kopieren, falls Export fehlschl√§gt
# Auf jede Datei klicken ‚Üí Inhalt kopieren ‚Üí In lokalen Bearbeiteor einf√ºgen

# 4. bolt.diy-Logs pr√ºfen
docker logs bolt | grep -i error
```

### Integration mit AI CoreKit Services

**bolt.diy + Supabase:**
- Vollst√§ndige CRUD-Apps mit Supabase-Backend generieren
- Automatische Datenbank-Schema-Erstellung
- Echtzeit-Subscriptions-Unterst√ºtzung
- Integrierte Auth-Integration

**bolt.diy + n8n:**
- Generierte APIs als n8n HTTP Request Ziele exportieren
- Benutzerdefinierte UI f√ºr n8n-Workflows bauen
- Admin-Dashboards f√ºr Workflow-Management erstellen

**bolt.diy + Ollama:**
- Lokale Modelle f√ºr Code-Generierung nutzen (falls Ollama aktiviert)
- Keine API-Kosten f√ºr Entwicklung
- Volle Privatsph√§re f√ºr sensible Projekte

**bolt.diy + ComfyUI:**
- Bildverarbeitungs-Oberfl√§chen generieren
- Benutzerdefinierte ComfyUI-Workflow-Bearbeiteoren bauen
- Galerien f√ºr generierte Bilder erstellen

### Ressourcen

- **Offizielles Repository**: [github.com/stackblitz-labs/bolt.diy](https://github.com/stackblitz-labs/bolt.diy)
- **Dokumentation**: [docs.bolt.new](https://docs.bolt.new) (bolt.new ist die gehostete Version)
- **Community-Beispiele**: Schau bei r/bolt_diy f√ºr Inspiration vorbei
- **Video-Tutorials**: Suche "bolt.diy tutorial" auf YouTube
- **Best Practices**: [github.com/stackblitz-labs/bolt.diy/discussions](https://github.com/stackblitz-labs/bolt.diy/discussions)

### Sicherheitshinweise

- **Keine Authentifizierung**: bolt.diy hat keine eingebaute Auth - gesch√ºtzt durch Caddy Basic Auth falls konfiguriert
- **API-Keys**: Committe niemals API-Keys in generierte Code-Repositories
- **√ñffentliches Deployment**: Generierte Apps k√∂nnen deine Prompts enthalten - √ºberpr√ºfe vor dem Teilen
- **Code-Review**: √úberpr√ºfe KI-generierten Code immer vor Produktiv-Nutzung
- **Umgebungsvariablen**: Nutze .env-Dateien f√ºr sensible Konfiguration
- **Nur HTTPS**: Greife nur √ºber HTTPS auf bolt.diy zu, um API-Keys w√§hrend der √úbertragung zu sch√ºtzen


---

# üé® OpenUI - UI-Komponenten-Generator</b> üß™</summary>

### Was ist OpenUI?

OpenUI ist ein **experimentelles** KI-gest√ºtztes Tool, das UI-Komponenten direkt aus Textbeschreibungen generiert. Es verwendet gro√üe Sprachmodelle, um React-, Vue-, Svelte- oder reine HTML-Komponenten basierend auf deinen Prompts zu erstellen. Obwohl es schnell Komponenten-Code erzeugen kann, variiert die Ausgabequalit√§t erheblich je nach verwendetem LLM-Modell.

**‚ö†Ô∏è Wichtig:** OpenUI ist experimentell und eignet sich am besten f√ºr Prototyping und Inspiration als f√ºr produktionsfertigen Code. F√ºr komplexe UI-Anforderungen oder vollst√§ndige Anwendungen solltest du stattdessen **bolt.diy** in Betracht ziehen.

### Funktionen

- **Multi-Framework-Unterst√ºtzung** - Generiere React-, Vue-, Svelte- oder HTML-Komponenten
- **Live-Vorschau** - Sieh Komponenten in Echtzeit rendern, w√§hrend sie generiert werden
- **KI-gest√ºtzt** - Verwendet Claude, GPT-4, Groq oder Ollama-Modelle
- **Kopieren/Exportieren** - Erhalte sauberen Code, bereit zum Einf√ºgen in dein Projekt
- **Styling-Optionen** - W√§hle zwischen Tailwind CSS, reinem CSS oder styled-components
- **Komponenten-Varianten** - Generiere mehrere Design-Optionen zum Vergleich
- **Schnelle Iteration** - Verfeinere Komponenten schnell mit Folge-Prompts

### Ersteinrichtung

**Erster Zugriff auf OpenUI:**

1. Navigiere zu `https://openui.deinedomain.com`
2. Kein Login erforderlich - OpenUI startet sofort
3. Konfiguriere deinen KI-Anbieter:
   - Klicke auf **Einstellungen** (Zahnrad-Symbol)
   - W√§hle Anbieter: OpenAI, Anthropic, Groq oder Ollama
   - Gib API-Schl√ºssel ein (falls externer Anbieter verwendet wird)
   - W√§hle Modell

**Empfohlene Modell-Konfiguration:**

| Anbieter | Modell | Qualit√§t | Geschwindigkeit | Kosten | Am besten f√ºr |
|----------|-------|---------|-----------------|--------|---------------|
| **Anthropic** | Claude 3.5 Sonnet | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Mittel | $$ | Produktionsreife Komponenten |
| **OpenAI** | GPT-4o | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Mittel | $$ | Komplexe Layouts, Barrierefreiheit |
| **OpenAI** | GPT-4o-mini | ‚≠ê‚≠ê‚≠ê‚≠ê | Schnell | $ | Schnelle Prototypen |
| **Groq** | llama-3.1-70b | ‚≠ê‚≠ê‚≠ê | Sehr schnell | $ | Schnelle Iteration |
| **Ollama** | Lokale Modelle | ‚≠ê‚≠ê | Variiert | Kostenlos | Privatsph√§re, Experimente |

**‚ö†Ô∏è Kritisch:** F√ºr beste Ergebnisse verwende **Claude 3.5 Sonnet** oder **GPT-4o**. Modelle niedrigerer Qualit√§t k√∂nnen unbrauchbare Komponenten erzeugen.

### Grundlegende Verwendung

**Generiere eine einfache Komponente:**

1. √ñffne `https://openui.deinedomain.com`
2. W√§hle Framework: **React**, **Vue**, **Svelte** oder **HTML**
3. Gib Beschreibung in die Prompt-Box ein:
   ```
   Moderne Preiskarte mit Gradientenhintergrund, 
   drei Stufen (Basic, Pro, Enterprise), 
   mit Funktionslisten und Call-to-Action-Buttons
   ```
4. Klicke auf **Generieren**
5. Warte 10-30 Sekunden f√ºr die Generierung
6. Sieh dir die Live-Vorschau auf der rechten Seite an
7. Klicke auf **Code kopieren**, um ihn in deinem Projekt zu verwenden

**Verfeinern mit Folge-Prompts:**

```
"Mach den Gradienten lila statt blau"
"F√ºge Hover-Animationen zu den Karten hinzu"
"Mach es mobile-responsive"
"F√ºge Icons f√ºr jede Funktion hinzu"
```

### Komponenten-Beispiele

#### Beispiel 1: Dashboard-Karte

```
Prompt: "Erstelle eine Dashboard-Statistik-Karte mit:
- Gro√üer Zahl (Metrik-Wert)
- Prozentuale √Ñnderung mit Aufw√§rts-/Abw√§rtspfeil
- Kleinem Diagramm/Sparkline
- Tooltip beim Hover
- Verwende Tailwind CSS mit shadcn/ui Design-Stil"

Framework: React
Modell: Claude 3.5 Sonnet

Ergebnis: Produktionsreife Komponente mit:
‚úì Ordentlichen TypeScript-Typen
‚úì Responsivem Design
‚úì Barrierefreiem Markup
‚úì Sauberem, kommentiertem Code
```

#### Beispiel 2: Formular-Komponente

```
Prompt: "Modernes Kontaktformular mit:
- Name-, E-Mail-, Nachricht-Feldern
- Echtzeit-Validierung
- Submit-Button mit Lade-Status
- Erfolgs-/Fehler-Toast-Benachrichtigungen
- Dark-Mode-Unterst√ºtzung"

Framework: React
Modell: GPT-4o

Ergebnis: Funktionale Formular-Komponente mit:
‚úì Formular-Validierung
‚úì Zustandsverwaltung
‚úì Fehlerbehandlung
‚úì Barrierefreiheitsfunktionen
```

#### Beispiel 3: Navigationsmen√º

```
Prompt: "Responsive Navigationsleiste:
- Logo links
- Men√ºpunkte in der Mitte
- Suchleiste und Profil-Avatar rechts
- Mobil: Hamburger-Men√º mit ausziehbarer Schublade
- Sticky beim Scrollen
- Glassmorphismus-Effekt"

Framework: Vue 3
Modell: Claude 3.5 Sonnet

Ergebnis: Vollst√§ndige Nav-Komponente mit:
‚úì Mobile Responsivit√§t
‚úì Fl√ºssigen Animationen
‚úì Vue 3 Composition API
‚úì Modernem Styling
```

### Integrationsmuster

**OpenUI + bolt.diy Workflow:**

1. **Komponenten in OpenUI generieren** - Schnelle UI-Mockups
2. **In bolt.diy kopieren** - In vollst√§ndige App integrieren
3. **Mit KI verfeinern** - bolt.diy f√ºr Funktionalit√§t verwenden
4. **Bereitstellen** - Vollst√§ndige Anwendung mit funktionierendem Backend

**OpenUI + n8n Workflow:**

```javascript
// OpenUI-generierte Komponenten als E-Mail-Templates verwenden

// 1. E-Mail-Template-Komponente in OpenUI generieren
Prompt: "Responsive E-Mail-Vorlage mit Header, Content-Bereich, 
         und Footer. Verwende Inline-CSS f√ºr E-Mail-Kompatibilit√§t."

// 2. HTML-Ausgabe kopieren

// 3. In n8n E-Mail-senden-Node verwenden
HTML: [OpenUI-generiertes HTML einf√ºgen]

// 4. Mit n8n-Variablen personalisieren
Betreff: Bestellbest√§tigung - {{ $json.orderId }}
Body: Platzhalter ersetzen mit {{ $json.customerName }}
```

### Best Practices

**Prompt-Engineering f√ºr OpenUI:**

‚úÖ **Tu:**
- Sei spezifisch √ºber Layout und Struktur
- Erw√§hne Framework-spezifische Muster (Hooks, Composables)
- Gib den Styling-Ansatz an (Tailwind, CSS-Module)
- Fordere explizit responsives Design an
- Bitte um Barrierefreiheitsfunktionen
- Erw√§hne Dark Mode falls ben√∂tigt

‚ùå **Nicht:**
- Vage Beschreibungen verwenden ("mach es sch√∂n")
- Komplexe Gesch√§ftslogik erwarten
- Annehmen, dass Zustandsverwaltung enthalten ist
- Backend-Integration anfordern
- Perfekten Code beim ersten Versuch erwarten

**Beispiele f√ºr gute Prompts:**

```
‚úì "Erstelle eine React-Komponente mit Tailwind CSS: 
   Karte mit Bild links (40%), Textinhalt rechts (60%), 
   CTA-Button unten, Hover-Effekt zum Anheben der Karte mit Schatten, 
   Mobil: Bild oben gestapelt"

‚úì "Vue 3 Composable f√ºr Formular-Validierung mit:
   - E-Mail-Validierung Regex
   - Passwort-St√§rke-Pr√ºfer  
   - Echtzeit-Fehlermeldungen
   - Gibt reaktiven Status und Validierungsfunktionen zur√ºck"

‚úì "Svelte-Komponente: Tab-Oberfl√§che mit 3 Tabs,
   fl√ºssige Slide-Animationen zwischen Inhalten,
   Indikatorlinie f√ºr aktiven Tab,
   Tastaturnavigation (Pfeiltasten),
   ARIA-Labels f√ºr Barrierefreiheit"
```

**Beispiele f√ºr schlechte Prompts:**

```
‚úó "Sch√∂nes Login-Formular"
‚úó "Dashboard"
‚úó "Mach es modern"
‚úó "Komponente wie Facebook"
```

### Einschr√§nkungen & bekannte Probleme

**Qualit√§t variiert je nach Modell:**

- **Claude 3.5 Sonnet / GPT-4o**: Durchgehend gut, produktionstauglich
- **GPT-4o-mini**: Gut f√ºr einfache Komponenten, kann bei komplexen Layouts Schwierigkeiten haben
- **Groq-Modelle**: Schnell, aber oft Code niedrigerer Qualit√§t
- **Ollama-Modelle**: Sehr inkonsistent, erfordert oft mehrere Versuche

**H√§ufige Probleme:**

1. **Unvollst√§ndige Komponenten** - Fehlende Imports, defektes JSX
2. **Nicht-funktionale Logik** - Zustandsverwaltung funktioniert nicht
3. **Schlechte Responsivit√§t** - Nur Desktop-Designs
4. **Barrierefreiheitsl√ºcken** - Fehlende ARIA-Labels, Tastaturnavigation
5. **Styling-Konflikte** - CSS-Spezifit√§tsprobleme

**Wann OpenUI verwenden:**

‚úÖ Gut f√ºr:
- Schnelle Komponenten-Mockups
- Design-Inspiration
- Lernen von Komponenten-Mustern
- Einfache, statische UI-Elemente
- E-Mail-Templates (HTML)

‚ùå Nicht gut f√ºr:
- Produktionsreife Komponenten ohne √úberpr√ºfung
- Komplexe Gesch√§ftslogik
- Vollst√§ndige Seitenlayouts
- Komponenten mit Backend-Integration
- Gesch√§ftskritische UI

### Fehlerbehebung

**Schlechte Ausgabequalit√§t:**

```bash
# 1. Zu einem besseren Modell wechseln
Einstellungen ‚Üí Anbieter: Anthropic
Modell: claude-3-5-sonnet-20241022

# 2. Prompt spezifischer machen
Statt: "Login-Formular"
Verwende: "React-Login-Formular mit E-Mail-Feld, Passwortfeld mit 
      Anzeigen/Verbergen-Umschaltung, Angemeldet-bleiben-Checkbox, Submit-Button 
      mit Lade-Status, Fehlermeldungs-Anzeige, 
      mit Tailwind CSS"

# 3. Mehrere Generierungen versuchen
Klicke 2-3 Mal auf "Generieren", w√§hle das beste Ergebnis

# 4. bolt.diy f√ºr komplexe Komponenten verwenden
OpenUI eignet sich am besten f√ºr einfache, isolierte Komponenten
```

**Komponente wird nicht gerendert:**

```bash
# 1. Browser-Konsole (F12) auf Fehler pr√ºfen

# 2. H√§ufige Probleme:
- Fehlende Imports: Erforderliche Dependencies hinzuf√ºgen
- JSX-Syntaxfehler: Klammer-Fehlanpassungen beheben
- CSS-Probleme: Pr√ºfen, ob Klassennamen korrekt sind

# 3. Code au√üerhalb von OpenUI testen
In CodeSandbox oder lokales Projekt kopieren
Dependencies manuell installieren
Mit ordentlichen Dev-Tools debuggen
```

**API-Fehler:**

```bash
# 1. API-Schl√ºssel verifizieren
Einstellungen ‚Üí API-Schl√ºssel ‚Üí Neu eingeben und speichern

# 2. API-Limits pr√ºfen
OpenAI: platform.openai.com/usage
Anthropic: console.anthropic.com

# 3. OpenUI-Logs pr√ºfen
docker logs openui --tail 50

# 4. Service bei Bedarf neu starten
docker compose restart openui
```

**Langsame Generierung:**

```bash
# 1. Zu schnellerem Modell wechseln
Groq: llama-3.1-70b (schnell, niedrigere Qualit√§t)
OpenAI: gpt-4o-mini (ausgewogen)

# 2. Prompt vereinfachen
Komplexe Komponenten in kleinere Teile aufteilen
Inkrementell generieren

# 3. OpenUI-Ressourcen pr√ºfen
docker stats openui
# Niedriger CPU/RAM? Server upgraden

# 4. Netzwerk zum KI-Anbieter pr√ºfen
# Langsame API-Antworten k√∂nnen anbieterseitig sein
```

### Alternative: bolt.diy

**Wenn OpenUI nicht ausreicht:**

Wenn OpenUI schlechte Qualit√§t generiert oder du brauchst:
- Vollst√§ndige Anwendungsentwicklung
- Backend-Integration
- Komplexe Zustandsverwaltung
- Mehrere verbundene Komponenten
- Produktionsfertigen Code

**‚Üí Verwende stattdessen bolt.diy:**
- Zuverl√§ssigere Code-Generierung
- Full-Stack-F√§higkeiten
- Besserer Iterations-Workflow
- Live-Entwicklungsumgebung
- Kann ganze Anwendungen generieren

Siehe [bolt.diy-Abschnitt](#ai-powered-development) f√ºr vollst√§ndige Dokumentation.

### Ressourcen

- **Offizielles Repository**: [github.com/wandb/openui](https://github.com/wandb/openui)
- **Dokumentation**: Begrenzt - Tool ist experimentell
- **Komponenten-Bibliotheken**: 
  - [shadcn/ui](https://ui.shadcn.com) - Muster f√ºr bessere Prompts kopieren
  - [Tailwind UI](https://tailwindui.com) - Inspiration f√ºr Designs
- **Alternative Tools**:
  - **bolt.diy** - Full-Stack-KI-Entwicklung
  - **v0.dev** - Vercels UI-Generator (extern)
  - **Lovable** - KI-App-Builder (extern)

### Sicherheit & Best Practices

**Code-√úberpr√ºfung erforderlich:**
- **√úberpr√ºfe generierten Code immer** vor Produktivnutzung
- Pr√ºfe auf Sicherheitsl√ºcken
- Validiere Eingabebehandlung
- Teste Barrierefreiheit
- Verifiziere responsives Verhalten

**API-Schl√ºssel-Sicherheit:**
- Verwende Umgebungsvariablen f√ºr API-Schl√ºssel
- Committen keine Schl√ºssel in Git
- Rotiere Schl√ºssel regelm√§√üig
- √úberwache API-Nutzung auf Anomalien

**Datenschutz-√úberlegungen:**
- Deine Prompts werden an KI-Anbieter gesendet (OpenAI, Anthropic, etc.)
- F√ºge keine sensible Gesch√§ftslogik in Prompts ein
- Verwende Ollama f√ºr private/sensible Projekte
- Generierter Code kann von KI-Anbietern protokolliert werden

**Lizenzierung:**
- Lizenzierung von KI-generiertem Code ist unklar
- √úberpr√ºfe die Bedingungen deines KI-Anbieters
- Ber√ºcksichtige rechtliche Auswirkungen f√ºr kommerzielle Nutzung
- Teste gr√ºndlich, als w√§re es Drittanbieter-Code

</details>

### KI-Agenten

<details>
<summary><b>ü§ñ Flowise - Visueller KI-Builder

### Was ist Flowise?

Flowise ist ein Open-Source visueller KI-Agenten-Builder, mit dem du anspruchsvolle KI-Anwendungen √ºber eine Drag-and-Drop-Oberfl√§che erstellen kannst. Aufgebaut auf LangChain erm√∂glicht es Entwicklern und Nicht-Entwicklern gleicherma√üen, Chatbots, Konversations-Agenten, RAG-Systeme und Multi-Agenten-Workflows ohne umfangreichen Code zu erstellen. Stell es dir vor wie "Figma f√ºr KI-Backend-Anwendungen".

### Funktionen

- **Visueller Workflow-Builder** - Drag-and-Drop-Oberfl√§che zum Erstellen von KI-Agenten und LLM-Flows
- **Multi-Agenten-Systeme** - Erstelle Teams spezialisierter KI-Agenten mit Supervisor-Koordination
- **RAG-Unterst√ºtzung** - Verbinde mit Dokumenten, Datenbanken und Wissensbasen f√ºr kontextbewusste Antworten
- **Tool-Calling** - Agenten k√∂nnen externe Tools, APIs und Funktionen dynamisch nutzen
- **Speicherverwaltung** - Konversations-Ged√§chtnis und Kontext-Beibehaltung √ºber Sitzungen hinweg
- **Mehrere LLM-Unterst√ºtzung** - Funktioniert mit OpenAI, Anthropic, Ollama, Groq und 50+ weiteren Anbietern
- **Vorgefertigte Vorlagen** - Starte mit fertigen Vorlagen f√ºr h√§ufige Anwendungsf√§lle
- **Assistenten-Modus** - Einsteigerfreundliche Methode zur Erstellung von KI-Agenten mit Datei-Upload-RAG
- **AgentFlow V2** - Erweiterte sequenzielle Workflows mit Schleifen, Bedingungen und Human-in-the-Loop
- **Streaming-Unterst√ºtzung** - Echtzeit-Antwort-Streaming f√ºr bessere UX
- **√úberall einbetten** - Generiere einbettbare Chat-Widgets f√ºr Websites

### Ersteinrichtung

**Erster Login bei Flowise:**

1. Navigiere zu `https://flowise.deinedomain.com`
2. **Erster Benutzer wird Admin** - Erstelle dein Konto
3. Setze ein starkes Passwort
4. Einrichtung abgeschlossen!

**Schnellstart:**

1. Klicke auf **Neu hinzuf√ºgen** ‚Üí W√§hle **Assistent** (am einfachsten) oder **Chatflow** (flexibel)
2. W√§hle eine Vorlage oder starte von Grund auf
3. F√ºge Nodes hinzu, indem du sie aus der linken Seitenleiste ziehst
4. Verbinde Nodes, um deinen Flow zu erstellen
5. Konfiguriere jeden Node (LLM, Prompts, Tools, etc.)
6. Klicke auf **Speichern**, dann **Bereitstellen**
7. Teste in der Chat-Oberfl√§che

### Drei Wege zum Erstellen in Flowise

**1. Assistent (Einsteigerfreundlich)**
- Einfache Oberfl√§che zum Erstellen von KI-Assistenten
- Lade Dateien f√ºr automatisches RAG hoch
- Befolge Anweisungen und verwende Tools
- Am besten f√ºr: Einfache Chatbots, Dokumenten-Q&A

**2. Chatflow (Flexibel)**
- Volle Kontrolle √ºber LLM-Ketten
- Erweiterte Techniken: Graph RAG, Reranker, Retriever
- Am besten f√ºr: Benutzerdefinierte Workflows, komplexe Logik

**3. AgentFlow (Am leistungsst√§rksten)**
- Multi-Agenten-Systeme mit Supervisor-Orchestrierung
- Sequenzielle Workflows mit Verzweigung
- Schleifen und Bedingungen
- Human-in-the-Loop-F√§higkeiten
- Am besten f√ºr: Komplexe Automatisierung, Enterprise-Workflows

### Deinen ersten Agenten erstellen

**Einfacher Chatbot mit RAG:**

1. **Neuen Assistenten erstellen:**
   - Klicke auf **Neu hinzuf√ºgen** ‚Üí **Assistent**
   - Benenne ihn: "Dokumenten-Q&A-Bot"

2. **Einstellungen konfigurieren:**
   - **Modell**: W√§hle `gpt-4o` (oder `llama3.2` √ºber Ollama)
   - **Anweisungen**: 
     ```
     Du bist ein hilfreicher Assistent, der Fragen basierend auf hochgeladenen Dokumenten beantwortet.
     Wenn du die Antwort nicht wei√üt, sag es - erfinde keine Informationen.
     ```

3. **Dokumente hochladen:**
   - Klicke auf **Dateien hochladen**
   - F√ºge PDF-, DOCX-, TXT-Dateien hinzu
   - Flowise erstellt automatisch Vektor-Embeddings

4. **Testen:**
   - Klicke auf **Chat**-Symbol
   - Frage: "Was sind die Hauptpunkte im hochgeladenen Dokument?"
   - Agent ruft relevante Chunks ab und antwortet

5. **Bereitstellen:**
   - Klicke auf **Bereitstellen**
   - Erhalte API-Endpunkt und Einbettungscode

### Multi-Agenten-Systeme

**Supervisor + Workers-Muster:**

Flowise unterst√ºtzt hierarchische Multi-Agenten-Systeme, bei denen ein Supervisor-Agent mehrere Worker-Agenten koordiniert:

```
Benutzeranfrage
    ‚Üì
Supervisor-Agent (koordiniert Aufgaben)
    ‚Üì
    ‚îú‚îÄ‚Üí Worker 1: Recherche-Agent (durchsucht Web)
    ‚îú‚îÄ‚Üí Worker 2: Analyse-Agent (analysiert Daten)
    ‚îî‚îÄ‚Üí Worker 3: Schreib-Agent (erstellt Berichte)
    ‚Üì
Supervisor aggregiert Ergebnisse
    ‚Üì
Endg√ºltige Antwort
```

**Ein Multi-Agenten-System erstellen:**

1. **Workers zuerst erstellen:**
   - Recherche-Agent: Google-Such-Tool hinzuf√ºgen
   - Analyse-Agent: Code-Interpreter-Tool hinzuf√ºgen
   - Schreib-Agent: Spezialisierter Prompt zum Schreiben

2. **Supervisor erstellen:**
   - **Supervisor-Agent**-Node hinzuf√ºgen
   - Alle Worker-Nodes verbinden
   - Delegationslogik konfigurieren

3. **Beispiel - Lead-Recherche-System:**
   - **Worker 1 (Lead-Researcher)**: Verwendet Google-Suche, um Firmeninfos zu finden
   - **Worker 2 (E-Mail-Schreiber)**: Erstellt personalisierte Outreach-E-Mails
   - **Supervisor**: Koordiniert Recherche ‚Üí E-Mail-Generierungs-Workflow

### n8n-Integration

**Flowise-Agenten von n8n aufrufen:**

Flowise stellt eine REST-API bereit, die n8n √ºber HTTP-Request-Nodes aufrufen kann.

**Flowise-API-Details abrufen:**

1. √ñffne in Flowise deinen bereitgestellten Chatflow/Agentflow
2. Klicke auf **API**-Tab
3. Kopiere:
   - **Endpunkt-URL**: `https://flowise.deinedomain.com/api/v1/prediction/{FLOW_ID}`
   - **API-Schl√ºssel**: Generiere in Einstellungen ‚Üí API-Schl√ºssel

**n8n HTTP Request-Konfiguration:**

```javascript
// HTTP Request Node
Methode: POST
URL: https://flowise.deinedomain.com/api/v1/prediction/{{FLOW_ID}}
Authentifizierung: Header Auth
  Header-Name: Authorization
  Header-Wert: Bearer {{YOUR_FLOWISE_API_KEY}}

Body (JSON):
{
  "question": "{{$json.user_query}}",
  "overrideConfig": {
    // Optional: Chatflow-Parameter √ºberschreiben
  }
}

// Antwort-Struktur:
{
  "text": "KI-Agenten-Antwort...",
  "chatId": "uuid-hier",
  "messageId": "uuid-hier"
}
```

### Beispiel-Workflows

#### Beispiel 1: Kundensupport-Automatisierung

**n8n ‚Üí Flowise-Integration:**

```javascript
// 1. Webhook-Trigger - Support-Ticket empfangen
// Eingabe: { "email": "kunde@beispiel.com", "issue": "Kann mich nicht anmelden" }

// 2. HTTP Request - Flowise Support-Agent abfragen
Methode: POST
URL: https://flowise.deinedomain.com/api/v1/prediction/support-agent-id
Header:
  Authorization: Bearer {{$env.FLOWISE_API_KEY}}
Body: {
  "question": "Kundenproblem: {{$json.issue}}. Gib L√∂sungsschritte an.",
  "overrideConfig": {
    "sessionId": "{{$json.email}}" // Konversationskontext beibehalten
  }
}

// 3. Code-Node - Flowise-Antwort parsen
const solution = $json.text;
return {
  customer: $('Webhook').item.json.email,
  issue: $('Webhook').item.json.issue,
  ai_solution: solution,
  resolved: solution.includes("gel√∂st") || solution.includes("behoben")
};

// 4. IF-Node - Pr√ºfen, ob automatisch gel√∂st
If: {{$json.resolved}} === true

// 5a. E-Mail senden - Automatisch gel√∂st
An: {{$json.customer}}
Betreff: Problem gel√∂st
Body: {{$json.ai_solution}}

// 5b. Ticket erstellen - Ben√∂tigt menschliche √úberpr√ºfung
// ‚Üí Baserow/Airtable-Node
```

#### Beispiel 2: Multi-Agenten-Recherche-Pipeline

**Komplett in Flowise erstellt, ausgel√∂st von n8n:**

```javascript
// In Flowise: Multi-Agenten-Recherche-System erstellen

// Agent 1: Web-Researcher
Tools: Google-Suche, Web-Scraper
Aufgabe: Informationen √ºber {{topic}} finden

// Agent 2: Datenanalyst  
Tools: Code-Interpreter
Aufgabe: Erkenntnisse analysieren und extrahieren

// Agent 3: Berichtsschreiber
Tools: Dokument-Generator
Aufgabe: Executive Summary erstellen

// Supervisor
Koordiniert: Recherche ‚Üí Analyse ‚Üí Schreiben
Gibt zur√ºck: Vollst√§ndigen Recherchebericht

// In n8n:
// 1. Zeitplan-Trigger - T√§glich um 9 Uhr

// 2. Code-Node - Recherche-Themen definieren
return [
  { topic: "KI-Automatisierungstrends 2025" },
  { topic: "LLM-Kostenoptimierungsstrategien" },
  { topic: "Enterprise-RAG-Implementierungen" }
];

// 3. HTTP Request - Flowise Multi-Agent aufrufen
// (Schleife √ºber Themen)
URL: https://flowise.deinedomain.com/api/v1/prediction/research-team-id
Body: {
  "question": "Recherchiere {{$json.topic}} und liefere umfassenden Bericht"
}

// 4. Google Drive - Berichte speichern
Dateiname: Recherche_{{$json.topic}}_{{$now}}.pdf
Inhalt: {{$json.text}}

// 5. Slack - Team benachrichtigen
Nachricht: "T√§gliche Rechercheberichte abgeschlossen: {{$json.length}} Themen"
```

#### Beispiel 3: RAG-Dokumenten-Q&A-System

**Flowise-Setup:**

1. **Chatflow mit RAG erstellen:**
   - **Document Loaders** hinzuf√ºgen: PDF, DOCX, Web-Scraper
   - **Text Splitter** hinzuf√ºgen: Recursive Character Splitter (Chunk-Gr√∂√üe: 1000)
   - **Embeddings** hinzuf√ºgen: OpenAI-Embeddings
   - **Vector Store** hinzuf√ºgen: Qdrant (intern: `http://qdrant:6333`)
   - **Retriever** hinzuf√ºgen: Vector Store Retriever (top k: 5)
   - **LLM Chain** hinzuf√ºgen: GPT-4o mit RAG-Prompt
   - Verbinden: Dokumente ‚Üí Splitter ‚Üí Embeddings ‚Üí Vector Store ‚Üí Retriever ‚Üí LLM

2. **Dokumente hochladen:**
   - Unternehmensrichtlinien, Produktdokumente, FAQs
   - Flowise verarbeitet und speichert in Qdrant

3. **Bereitstellen & API-Schl√ºssel erhalten**

**n8n-Integration:**

```javascript
// 1. Slack-Trigger - Bei Nachricht im #fragen-Kanal

// 2. HTTP Request - Flowise RAG abfragen
URL: https://flowise.deinedomain.com/api/v1/prediction/rag-chatbot-id
Body: {
  "question": "{{$json.text}}"
}

// 3. Slack-Antwort
Antwort im Thread: {{$json.text}}
Nachricht: {{$json.response}}
Zitate: {{$json.sourceDocuments}}
```

### Erweiterte Funktionen

**AgentFlow V2 (Sequenzielle Workflows):**

- **Tool-Node**: F√ºhre spezifische Tools deterministisch aus
- **Bedingungs-Node**: Verzweigungslogik basierend auf Ausgaben
- **Schleifen-Node**: Iteriere √ºber Ergebnisse
- **Variablen-Node**: Speichere und rufe Status ab
- **SubFlow-Node**: Rufe andere Flowise-Flows als Module auf

**Beispiel - Rechnungsverarbeitungs-Flow:**

```
Start
  ‚Üì
Tool-Node: Text aus PDF-Rechnung extrahieren
  ‚Üì
LLM-Node: Rechnungsdaten parsen (Betrag, Datum, Lieferant)
  ‚Üì
Bedingungs-Node: Betrag > 1000‚Ç¨?
  ‚îú‚îÄ Ja ‚Üí SubFlow: Genehmigungsworkflow
  ‚îî‚îÄ Nein ‚Üí Tool-Node: Auto-Genehmigung
  ‚Üì
Tool-Node: Buchhaltungssystem aktualisieren
  ‚Üì
Ende
```

### Best Practices

**Prompt-Engineering:**
- Sei spezifisch in System-Anweisungen
- F√ºge Beispiele gew√ºnschter Ausgaben hinzu
- Definiere Verhalten f√ºr Grenzf√§lle
- Verwende Variablen f√ºr dynamische Inhalte

**RAG-Optimierung:**
- Chunk-Gr√∂√üe: 500-1500 Zeichen (abh√§ngig vom Anwendungsfall)
- √úberlappung: 10-20% f√ºr besseren Kontext
- Top-K-Abruf: 3-7 Chunks
- Verwende Metadaten-Filterung wenn m√∂glich
- Aktualisiere Vector Store regelm√§√üig mit neuen Dokumenten

**Multi-Agenten-Design:**
- Halte Worker-Agenten spezialisiert (Single Responsibility)
- Supervisor sollte klare Delegationsregeln haben
- Teste Agenten einzeln vor dem Kombinieren
- √úberwache Token-Nutzung pro Agent

**Performance:**
- Verwende Streaming f√ºr bessere UX
- Cache Embeddings wenn m√∂glich
- Setze angemessene Timeout-Limits
- Implementiere Rate-Limiting f√ºr √∂ffentliche Endpunkte

### Fehlerbehebung

**Agent antwortet nicht:**

```bash
# 1. Pr√ºfe, ob Flowise l√§uft
docker ps | grep flowise

# 2. Logs pr√ºfen
docker logs flowise -f

# 3. API-Schl√ºssel verifizieren
# In Flowise: Einstellungen ‚Üí API-Schl√ºssel ‚Üí Pr√ºfe ob Schl√ºssel g√ºltig ist

# 4. Mit curl testen
curl -X POST https://flowise.deinedomain.com/api/v1/prediction/YOUR_FLOW_ID \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"question": "Hallo"}'
```

**RAG findet Dokumente nicht:**

```bash
# 1. Pr√ºfe, ob Dokumente verarbeitet wurden
# In Flowise: Flow √∂ffnen ‚Üí Vector-Store-Node pr√ºfen ‚Üí Gespeicherte Dokumente ansehen

# 2. Qdrant-Status verifizieren
docker ps | grep qdrant
curl http://localhost:6333/health

# 3. Embeddings-Modell pr√ºfen
# Stelle sicher, dass OpenAI-API-Schl√ºssel gesetzt ist oder Ollama f√ºr lokale Embeddings l√§uft

# 4. Abruf direkt testen
# In Flowise: Testmodus ‚Üí Frage stellen ‚Üí "Source Documents" in Antwort pr√ºfen

# 5. Abruf-Einstellungen anpassen
# Top-K-Wert erh√∂hen (versuche 5-10)
# √Ñhnlichkeitsschwelle senken
# Verschiedene Chunk-Gr√∂√üen ausprobieren
```

**Multi-Agenten-Fehler:**

```bash
# 1. Worker-Agenten einzeln pr√ºfen
# Teste jeden Worker separat vor Supervisor

# 2. Tool-Verf√ºgbarkeit verifizieren
# Pr√ºfe, ob Tools (Google-Suche, APIs) mit g√ºltigen Credentials konfiguriert sind

# 3. LLM-Unterst√ºtzung f√ºr Function Calling pr√ºfen
# Nicht alle Modelle unterst√ºtzen Tool Calling - verwende GPT-4o, Claude 3.5 oder Mistral

# 4. Supervisor-Prompt √ºberpr√ºfen
# Stelle sicher, dass Supervisor klare Anweisungen hat, wann welcher Worker zu verwenden ist

# 5. Logs auf spezifische Fehler √ºberwachen
docker logs flowise | grep -i error
```

**n8n-Integrationsprobleme:**

```bash
# 1. Flowise-API-Endpunkt verifizieren
# Genaue URL im Flowise-API-Tab pr√ºfen

# 2. Authentifizierung testen
# API-Schl√ºssel in Flowise neu generieren bei 401/403-Fehlern

# 3. Request-Format pr√ºfen
# Body muss JSON mit "question"-Feld sein

# 4. CORS bei Bedarf aktivieren
# Setze CORS_ORIGINS-Umgebungsvariable in Flowise

# 5. n8n HTTP Request Timeout pr√ºfen
# Timeout f√ºr lang laufende Agenten erh√∂hen (60-120 Sekunden)
```

**Langsame Performance:**

```bash
# 1. Modellgeschwindigkeit pr√ºfen
# GPT-4o: Langsam aber akkurat
# GPT-4o-mini: Schneller, gute Qualit√§t
# Groq: Sehr schnell (versuche llama-3.1-70b)

# 2. RAG-Abruf optimieren
# Top-K-Wert reduzieren
# Kleinere Embedding-Modelle verwenden

# 3. Streaming aktivieren
# In Flowise-Chatflow-Einstellungen: Streaming-Antworten aktivieren

# 4. Flowise-Ressourcen √ºberwachen
docker stats flowise
# Hohe CPU/Memory? Server upgraden oder gleichzeitige Anfragen reduzieren

# 5. Caching verwenden
# Konversations-Ged√§chtnis-Caching aktivieren
# Embeddings f√ºr h√§ufig abgerufene Dokumente cachen
```

### Integration mit AI CoreKit-Services

**Flowise + Qdrant:**
- Verwende Qdrant als Vector Store f√ºr RAG
- Interne URL: `http://qdrant:6333`
- Erstelle Collections in Qdrant UI, referenziere sie in Flowise

**Flowise + Ollama:**
- Verwende lokale LLMs statt OpenAI
- Ollama Chat Models-Node hinzuf√ºgen
- Basis-URL: `http://ollama:11434`
- Modelle: llama3.2, mistral, qwen2.5-coder

**Flowise + n8n:**
- n8n l√∂st Flowise-Agenten √ºber API aus
- Flowise kann n8n-Webhooks als Tools aufrufen
- Bidirektionale Integration f√ºr komplexe Workflows

**Flowise + Open WebUI:**
- Beide k√∂nnen dasselbe Ollama-Backend verwenden
- Flowise f√ºr agentische Workflows
- Open WebUI f√ºr einfache Chat-Oberfl√§che

### Ressourcen

- **Offizielle Website**: [flowiseai.com](https://flowiseai.com)
- **Dokumentation**: [docs.flowiseai.com](https://docs.flowiseai.com)
- **GitHub**: [github.com/FlowiseAI/Flowise](https://github.com/FlowiseAI/Flowise)
- **Marketplace**: Vorgefertigte Vorlagen und Flows in Flowise UI
- **Community**: [Discord](https://discord.gg/jbaHfsRVBW)
- **YouTube-Tutorials**: Suche "Flowise tutorial" f√ºr Video-Anleitungen
- **Vorlagen-Bibliothek**: Integrierte Vorlagen in Flowise f√ºr h√§ufige Anwendungsf√§lle

### Sicherheitshinweise

- **Authentifizierung erforderlich**: Richte API-Schl√ºssel f√ºr Produktion ein
- **Rate-Limiting**: Implementiere Rate-Limits f√ºr √∂ffentliche Endpunkte
- **API-Schl√ºssel-Verwaltung**: Speichere Schl√ºssel in Umgebungsvariablen, niemals hardcoden
- **CORS-Konfiguration**: Konfiguriere CORS_ORIGINS f√ºr Web-Einbettungen
- **Datenschutz**: In RAG hochgeladene Dokumente werden in Vector-DB gespeichert
- **LLM-API-Schl√ºssel**: Halte OpenAI/Anthropic-Schl√ºssel sicher
- **Zugriffskontrolle**: Beschr√§nke Flowise-Dashboard-Zugriff auf vertrauensw√ºrdige Benutzer

### Preise & Ressourcen

**Ressourcenanforderungen:**
- **Basis-Chatbot**: 2GB RAM, minimale CPU
- **RAG-System**: 4GB RAM, moderate CPU (f√ºr Embeddings)
- **Multi-Agent**: 8GB+ RAM, h√∂here CPU
- **Mit Ollama**: +8GB RAM pro LLM-Modell

**API-Kosten (bei Verwendung externer LLMs):**
- OpenAI: ~0,01‚Ç¨ pro Konversation mit GPT-4o-mini
- Anthropic: ~0,025‚Ç¨ pro Konversation mit Claude 3.5 Sonnet
- Groq: Kostenloses Kontingent verf√ºgbar, dann nutzungsbasiert
- Ollama: Kostenlos (selbst gehostet)

**Kostenoptimierung:**
- Verwende Ollama f√ºr Entwicklung/Testing
- Wechsle zu externen APIs f√ºr Produktionsqualit√§t
- Implementiere Caching zur Reduzierung von API-Aufrufen
- Verwende g√ºnstigere Modelle (GPT-4o-mini) wenn m√∂glich
