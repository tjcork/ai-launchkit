# üîä OpenedAI-Speech - Text-zu-Sprache

### Was ist OpenedAI-Speech?

OpenedAI-Speech ist ein selbst gehosteter Text-zu-Sprache-Dienst, der OpenAI-kompatible API-Endpunkte bereitstellt, angetrieben von Piper TTS. Er bietet hochwertige, nat√ºrlich klingende Stimmen in mehreren Sprachen mit vollst√§ndiger Privatsph√§re - die gesamte Audio-Generierung erfolgt auf deinem Server, ohne Daten an externe Dienste zu senden.

### Funktionen

- **OpenAI-kompatible API**: Direkter Ersatz f√ºr OpenAIs TTS-API (`/v1/audio/speech`)
- **Mehrere Stimmenmodelle**: Vorkonfigurierte englische Stimmen (alloy, echo, fable, onyx, nova, shimmer)
- **Mehrsprachige Unterst√ºtzung**: F√ºge Stimmen in √ºber 50 Sprachen hinzu, darunter Deutsch, Franz√∂sisch, Spanisch, Italienisch und mehr
- **Schnelle Generierung**: ~2-5 Sekunden pro Satz auf CPU, <1 Sekunde mit GPU
- **Privatsph√§re zuerst**: Die gesamte Audio-Generierung erfolgt lokal auf deinem Server
- **Automatischer Modell-Download**: Stimmenmodelle werden beim ersten Gebrauch automatisch heruntergeladen

### Erste Einrichtung

**OpenedAI-Speech ist intern bereitgestellt (kein direkter Webzugriff):**

- **Interne URL**: `http://openedai-speech:8000`
- **API-Endpunkt**: `/v1/audio/speech`
- **Authentifizierung**: Bearer-Token (Dummy-Token akzeptiert: `sk-dummy`)
- **Stimmenmodelle**: Werden beim ersten Gebrauch automatisch heruntergeladen

**Vorkonfigurierte englische Stimmen:**
- `alloy` - Neutrale, ausgewogene Stimme
- `echo` - M√§nnliche, selbstbewusste Stimme
- `fable` - Britische, narrative Stimme
- `onyx` - Tiefe, autoritative Stimme
- `nova` - Weibliche, energiegeladene Stimme
- `shimmer` - Sanfte, warme Stimme

### n8n-Integration einrichten

**Keine Anmeldedaten erforderlich** - OpenedAI-Speech wird √ºber den HTTP-Request-Node mit Dummy-Authentifizierung aufgerufen.

**Interne URL:** `http://openedai-speech:8000`

### Beispiel-Workflows

#### Beispiel 1: Einfache Text-zu-Sprache

```javascript
// 1. Trigger-Node (Webhook, Zeitplan, etc.)
// Eingabetext zur Umwandlung in Sprache

// 2. HTTP-Request-Node - Sprache generieren
Methode: POST
URL: http://openedai-speech:8000/v1/audio/speech

Header:
  - Name: Content-Type
    Wert: application/json
  - Name: Authorization
    Wert: Bearer sk-dummy

Send Body: JSON
{
  "model": "tts-1",
  "input": "{{ $json.text }}",
  "voice": "alloy",
  "response_format": "mp3"
}

Response Format: File
Put Output in Field: data

// 3. Aktions-Node - Audio verwenden
// In Datei speichern, per E-Mail senden, in Cloud hochladen, etc.
```

#### Beispiel 2: Mehrsprachige Sprachantwort

```javascript
// Text-zu-Sprache auf Deutsch oder Englisch

// 1. Webhook-Trigger - Empfange Text + Sprache
// Eingabe: { "text": "Hallo Welt", "language": "de" }

// 2. IF-Node - Pr√ºfe Sprache
If: {{ $json.language }} === 'de'

// 3a. HTTP-Request - Deutsche Stimme
Methode: POST
URL: http://openedai-speech:8000/v1/audio/speech
Header:
  Content-Type: application/json
  Authorization: Bearer sk-dummy
Body: {
  "model": "tts-1",
  "input": "{{ $json.text }}",
  "voice": "thorsten"  // Deutsche m√§nnliche Stimme
}

// 3b. HTTP-Request - Englische Stimme
Methode: POST
URL: http://openedai-speech:8000/v1/audio/speech
Body: {
  "model": "tts-1",
  "input": "{{ $json.text }}",
  "voice": "alloy"
}

// 4. HTTP-Response - Audio-Datei zur√ºckgeben
Antwort: Binary
Binary Property: data
```

#### Beispiel 3: Automatisierte Podcast-Generierung

```javascript
// Generiere Audio-Podcast aus Blog-Beitr√§gen

// 1. RSS-Feed-Trigger - Neuer Blog-Beitrag ver√∂ffentlicht
// Oder Zeitplan-Trigger + RSS abrufen

// 2. HTTP-Request - Artikel-Inhalt abrufen
Methode: GET
URL: {{ $json.link }}

// 3. HTML-Extrahierungs-Node - Haupttext extrahieren
Selector: article, .post-content, main
Output: text

// 4. Code-Node - Text bereinigen und formatieren
const text = $input.item.json.text;

// √úberfl√ºssige Leerzeichen entfernen
const cleaned = text.replace(/\s+/g, ' ').trim();

// In Abschnitte aufteilen (Piper hat ~500 Zeichen-Limit pro Anfrage)
const chunks = [];
const sentences = cleaned.match(/[^.!?]+[.!?]+/g) || [cleaned];
let currentChunk = '';

for (const sentence of sentences) {
  if ((currentChunk + sentence).length < 450) {
    currentChunk += sentence;
  } else {
    if (currentChunk) chunks.push(currentChunk);
    currentChunk = sentence;
  }
}
if (currentChunk) chunks.push(currentChunk);

return chunks.map(chunk => ({ text: chunk }));

// 5. Loop-Node - Jeden Abschnitt verarbeiten
Items: {{ $json }}

// 6. HTTP-Request - Sprache f√ºr Abschnitt generieren
Methode: POST
URL: http://openedai-speech:8000/v1/audio/speech
Header:
  Content-Type: application/json
  Authorization: Bearer sk-dummy
Body: {
  "model": "tts-1",
  "input": "{{ $json.text }}",
  "voice": "fable"  // Britische Erz√§hler-Stimme
}

// 7. Code-Node - Audio-Dateien zusammenf√ºgen
// Verwende FFmpeg, um alle Audio-Abschnitte zu verbinden

// 8. In Cloud-Speicher hochladen - Finale Podcast-Audio
// Google Drive, S3, Dropbox, etc.

// 9. WordPress/Ghost aktualisieren - Audio-Player zum Beitrag hinzuf√ºgen
```

#### Beispiel 4: Sprachaktivierte Kundenbenachrichtigungen

```javascript
// Sprachnachrichten an Kunden senden

// 1. Webhook-Trigger - Bestellstatus-Update
// Eingabe: { "customer_phone": "+491234567890", "status": "shipped", "order_id": "12345" }

// 2. Set-Node - Benachrichtigungsnachricht erstellen
const messages = {
  shipped: `Deine Bestellung ${$json.order_id} wurde versandt! Verfolge dein Paket auf unserer Website.`,
  delivered: `Gro√üartig! Deine Bestellung ${$json.order_id} wurde zugestellt. Viel Freude mit deinem Kauf!`,
  delayed: `Entschuldigung, aber Bestellung ${$json.order_id} hat sich verz√∂gert. Wir informieren dich bald.`
};

return {
  phone: $json.customer_phone,
  message: messages[$json.status] || 'Bestellungs-Update verf√ºgbar'
};

// 3. HTTP-Request - Sprachnachricht generieren
Methode: POST
URL: http://openedai-speech:8000/v1/audio/speech
Header:
  Content-Type: application/json
  Authorization: Bearer sk-dummy
Body: {
  "model": "tts-1",
  "input": "{{ $json.message }}",
  "voice": "nova"  // Freundliche weibliche Stimme
}

// 4. Twilio-Node - Sprachanruf t√§tigen
Action: Make Call
To: {{ $json.phone }}
URL: [URL zur gehosteten Audio-Datei]

// Oder WhatsApp/Telegram mit Audio-Nachricht
```

### Deutsche Stimmen hinzuf√ºgen (oder andere Sprachen)

OpenedAI-Speech verwendet Piper TTS, welches √ºber 50 Sprachen unterst√ºtzt. So f√ºgst du deutsche Stimmen hinzu:

**Schritt 1: Stimmen-Konfiguration bearbeiten**

```bash
# Auf deinen Server zugreifen
ssh user@deinedomain.com

# Zum AI CoreKit navigieren
cd ~/ai-corekit

# Stimmen-Konfiguration bearbeiten
nano openedai-config/voice_to_speaker.yaml
```

**Schritt 2: Deutsche Stimmen hinzuf√ºgen**

Finde den `tts-1`-Abschnitt und f√ºge deutsche Stimmen hinzu:

```yaml
tts-1:
  # Bestehende englische Stimmen...
  alloy:
    model: en_US-amy-medium
    speaker: # Standard-Sprecher
  
  # Deutsche Stimmen unten hinzuf√ºgen:
  thorsten:
    model: de_DE-thorsten-medium
    speaker: # Standard-Sprecher
  eva:
    model: de_DE-eva_k-x_low
    speaker: # Standard-Sprecher
  kerstin:
    model: de_DE-kerstin-low
    speaker: # Standard-Sprecher
```

**Schritt 3: Dienst neu starten**

```bash
docker compose restart openedai-speech
```

**Schritt 4: Deutsche Stimmen in n8n verwenden**

```javascript
// HTTP-Request-Node
Methode: POST
URL: http://openedai-speech:8000/v1/audio/speech
Header:
  Content-Type: application/json
  Authorization: Bearer sk-dummy
Body: {
  "model": "tts-1",
  "input": "Hallo, dies ist ein Test der deutschen Sprachausgabe.",
  "voice": "thorsten"  // Hochwertige deutsche m√§nnliche Stimme
}
```

**Verf√ºgbare deutsche Stimmen:**

| Stimme | Geschlecht | Qualit√§t | Geschwindigkeit | Am besten f√ºr |
|-------|--------|---------|-------|----------|
| `thorsten` | M√§nnlich | Medium | Ausgewogen | Allgemeine Nutzung, professionell |
| `eva` | Weiblich | X-Low | Sehr schnell | Schnelle Benachrichtigungen |
| `kerstin` | Weiblich | Low | Schnell | Lockere Inhalte |

**Mehr Stimmen verf√ºgbar unter:** https://rhasspy.github.io/piper-samples/

### Andere Sprachen hinzuf√ºgen

Der gleiche Prozess funktioniert f√ºr jede von Piper unterst√ºtzte Sprache:

**Beliebte Sprachcodes:**
- Deutsch: `de_DE`
- Franz√∂sisch: `fr_FR`
- Spanisch: `es_ES`
- Italienisch: `it_IT`
- Portugiesisch: `pt_BR`
- Niederl√§ndisch: `nl_NL`
- Polnisch: `pl_PL`
- Russisch: `ru_RU`

**Beispiel: Franz√∂sische Stimme hinzuf√ºgen**

```yaml
tts-1:
  # Franz√∂sische Stimme
  marie:
    model: fr_FR-siwis-medium
    speaker: # Standard-Sprecher
```

Dienst neu starten und verwenden: `"voice": "marie"`

### Stimmenmodell-Download

**Modelle werden automatisch beim ersten Gebrauch heruntergeladen:**

1. Erste Anfrage mit einer neuen Stimme l√∂st Download aus
2. Download-Zeit: ~30-90 Sekunden pro Stimme
3. Modelle werden dauerhaft zwischengespeichert (~20-100MB pro Stimme)
4. Nachfolgende Anfragen sind sofort

**Download-Fortschritt pr√ºfen:**

```bash
docker logs openedai-speech -f
```

### Antwort-Formate

OpenedAI-Speech unterst√ºtzt mehrere Audio-Formate:

**Verf√ºgbare Formate:**
- `mp3` - Komprimiert, kleine Dateigr√∂√üe (Standard)
- `opus` - Hohe Qualit√§t, effiziente Kompression
- `aac` - Gute Qualit√§t, breite Kompatibilit√§t
- `flac` - Verlustfrei, gro√üe Dateigr√∂√üe
- `wav` - Unkomprimiert, beste Qualit√§t, sehr gro√ü
- `pcm` - Rohe Audio-Daten

**Format in Anfrage angeben:**

```json
{
  "model": "tts-1",
  "input": "Hallo Welt",
  "voice": "alloy",
  "response_format": "opus"
}
```

### Fehlerbehebung

**Problem 1: Dienst antwortet nicht**

```bash
# Dienststatus pr√ºfen
docker ps | grep openedai-speech

# Sollte zeigen: STATUS = Up

# Logs pr√ºfen
docker logs openedai-speech --tail 50

# Bei Bedarf neu starten
docker compose restart openedai-speech
```

**L√∂sung:**
- Stelle sicher, dass der Dienst l√§uft: `docker ps | grep openedai-speech`
- Pr√ºfe auf Port-Konflikte (Port 8000 wird von Supabase Kong verwendet)
- OpenedAI-Speech verwendet Port 8000 intern (√ºber Dienstname erreichbar)

**Problem 2: Stimme nicht gefunden-Fehler**

```bash
# Verf√ºgbare Stimmen pr√ºfen
docker exec openedai-speech cat /app/config/voice_to_speaker.yaml

# Schreibweise des Stimmennamens √ºberpr√ºfen
# Stimmennamen sind gro√ü-/kleinschreibungssensitiv!
```

**L√∂sung:**
- Stimmennamen m√ºssen exakt √ºbereinstimmen (gro√ü-/kleinschreibungssensitiv)
- Pr√ºfe `voice_to_speaker.yaml` f√ºr konfigurierte Stimmen
- Standard-Stimmen: alloy, echo, fable, onyx, nova, shimmer
- Benutzerdefinierte Stimmen: M√ºssen zur Konfigurationsdatei hinzugef√ºgt werden

**Problem 3: Erste Anfrage ist sehr langsam**

```bash
# Modell-Download √ºberwachen
docker logs openedai-speech -f

# Du wirst sehen:
# Downloading voice model: en_US-amy-medium
# Progress: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100%
```

**L√∂sung:**
- Erste Anfrage l√§dt Stimmenmodell herunter (~30-90 Sekunden)
- Nachfolgende Anfragen werden in 2-5 Sekunden abgeschlossen
- Modelle werden dauerhaft zwischengespeichert
- Lade Modelle vorab herunter, indem du jede Stimme nach der Einrichtung testest

**Problem 4: Deutsche Stimme klingt falsch**

```bash
# Stimmen-Konfiguration pr√ºfen
docker exec openedai-speech cat /app/config/voice_to_speaker.yaml | grep -A 2 thorsten

# Sollte zeigen:
# thorsten:
#   model: de_DE-thorsten-medium
#   speaker:
```

**L√∂sung:**
- Stelle korrekten Modellcode sicher: `de_DE-thorsten-medium` (nicht `en_US`)
- Stimme muss zu `voice_to_speaker.yaml` hinzugef√ºgt werden
- Dienst nach Konfigurations√§nderungen neu starten
- √úberpr√ºfe, ob Sprachcode zum Stimmenmodell passt

**Problem 5: Audio-Qualit√§t ist schlecht**

**L√∂sung:**
- Verwende h√∂herwertige Stimmenmodelle:
  - `*-low` ‚Üí `*-medium` ‚Üí `*-high`
- Wechsle zu unkomprimiertem Format: `"response_format": "wav"`
- Probiere verschiedene Stimmen aus (einige sind qualitativ hochwertiger)
- F√ºr beste Qualit√§t: Verwende Medium- oder High-Quality-Modelle
- Beispiel: `en_US-libritts-high` (beste englische Qualit√§t)

**Problem 6: Kein Zugriff von n8n**

```bash
# Verbindung vom n8n-Container testen
docker exec n8n curl http://openedai-speech:8000/

# Sollte Health-Check oder Fehlerseite zur√ºckgeben

# Tats√§chlichen TTS-Endpunkt testen
docker exec n8n curl -X POST http://openedai-speech:8000/v1/audio/speech \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-dummy" \
  -d '{"model":"tts-1","input":"test","voice":"alloy"}'
```

**L√∂sung:**
- Verwende interne URL: `http://openedai-speech:8000` (nicht localhost oder IP)
- Stelle sicher, dass beide Dienste im gleichen Docker-Netzwerk sind
- F√ºge Dummy-Authorization-Header hinzu: `Bearer sk-dummy`
- Pr√ºfe, ob Dienst l√§uft: `docker ps | grep openedai-speech`

### Ressourcen

- **GitHub**: https://github.com/matatonic/openedai-speech
- **Piper TTS**: https://github.com/rhasspy/piper
- **Stimmen-Beispiele**: https://rhasspy.github.io/piper-samples/
- **OpenAI TTS API-Referenz**: https://platform.openai.com/docs/api-reference/audio/createSpeech
- **Verf√ºgbare Sprachen**: √úber 50 Sprachen unterst√ºtzt

### Best Practices

**F√ºr beste Audio-Qualit√§t:**

1. **W√§hle die richtige Stimmen-Qualit√§t:**
   - Entwicklung/Testing: Low-Quality (schnell, klein)
   - Produktion: Medium-Quality (ausgewogen)
   - Premium: High-Quality (langsam, gro√ü)

2. **Optimiere Text-Eingabe:**
   - Halte S√§tze unter 500 Zeichen
   - Verwende korrekte Interpunktion f√ºr nat√ºrliche Pausen
   - Teile langen Text in Abschnitte auf
   - F√ºge Kommas f√ºr nat√ºrliche Geschwindigkeit hinzu

3. **Behandle Fehler elegant:**
   - Wiederhole bei Netzwerkfehlern
   - Validiere Textl√§nge vor dem Senden
   - Speichere generierte Audiodaten zwischen, um Neugenerierung zu vermeiden
   - Setze sinnvolle Timeouts (10-30 Sekunden)

4. **Leistungsoptimierung:**
   - Lade h√§ufig verwendete Stimmenmodelle vorab herunter
   - Verwende niedrigere Qualit√§t f√ºr Echtzeit-Apps
   - Verarbeite mehrere Anfragen im Batch
   - Speichere Ergebnisse f√ºr wiederholte Phrasen zwischen

5. **Mehrsprachige Unterst√ºtzung:**
   - Konfiguriere Stimmen f√ºr alle ben√∂tigten Sprachen vorab
   - Teste jede Stimme vor Produktiveinsatz
   - Ber√ºcksichtige regionale Akzente (US vs UK Englisch)
   - Verwende sprachspezifische Stimmen f√ºr beste Qualit√§t

**Wann OpenedAI-Speech verwenden:**

- ‚úÖ Sprachbenachrichtigungen und Alarme
- ‚úÖ H√∂rbuch- und Podcast-Generierung
- ‚úÖ Sprachassistenten und Chatbots
- ‚úÖ Barrierefreiheit-Funktionen (Text-zu-Sprache)
- ‚úÖ Mehrsprachiger Inhalt
- ‚úÖ Telefonsystem-IVR-Nachrichten
- ‚úÖ Bildungsinhalte
- ‚ùå Echtzeit mit niedriger Latenz (<100ms) - verwende stattdessen Chatterbox
- ‚ùå Emotionale Ausdruckskontrolle - verwende stattdessen Chatterbox
- ‚ùå Stimmen-Klonen - verwende stattdessen Chatterbox

### Integration mit anderen Diensten

**Sprache-zu-Sprache-Pipeline:**

```
Faster-Whisper (STT) ‚Üí LLM (Verarbeitung) ‚Üí OpenedAI-Speech (TTS)
```

**Kompletter Workflow:**
1. Benutzer sendet Sprachnachricht
2. Faster-Whisper transkribiert zu Text
3. LLM (GPT/Claude/Ollama) verarbeitet Anfrage
4. OpenedAI-Speech wandelt Antwort in Audio um
5. Sende Audio zur√ºck an Benutzer

**Beispiel-Plattformen:**
- Telegram Sprachnachrichten
- WhatsApp Audio-Nachrichten
- Telefonsysteme (Twilio)
- Discord Sprach-Bots
- Web-Apps mit Audio
