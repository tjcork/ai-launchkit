# CLAUDE.md

This file provides guidance to Claude (claude.ai) and other AI assistants when working with code in this repository.

## Service Specifications
*   **Service Structure**: All services must adhere to the structure defined in [docs/SERVICE_STRUCTURE_SPEC.md](docs/SERVICE_STRUCTURE_SPEC.md). This includes file locations for `prepare.sh`, `startup.sh`, `secrets.sh`, and `docker-compose.yml`.

## Project Overview

This is **AI LaunchKit**: a comprehensive Docker Compose-based toolkit that creates a complete self-hosted AI development and automation environment. It transforms any Ubuntu server into a powerful AI development platform with 20+ pre-configured services that can be selectively deployed with a single command.

### Core Features
- **AI Development Tools**: bolt.diy, OpenHands, OpenUI, ComfyUI, Dify
- **Automation Platform**: n8n with 300+ pre-configured workflows, Flowise
- **LLM Infrastructure**: Ollama, Open WebUI, Letta
- **Vector Databases**: Qdrant, Supabase (with pgvector)
- **Monitoring & Observability**: Langfuse, Grafana, Prometheus
- **Media Processing**: ComfyUI (Stable Diffusion), Speech Stack (Whisper STT, OpenedAI TTS)
- **Development Tools**: SearXNG, Crawl4ai, browserless (planned), LiveKit (planned)

## Essential Commands

### Installation and Updates
```bash
# Main installation (from project root)
launchkit init
launchkit config
launchkit up

# Update all services to latest versions
launchkit update
```

### Docker Operations
```bash
# View running services
launchkit ps

# View logs for specific service
launchkit logs [service-name]

# Restart services
launchkit restart

# Stop all services
launchkit down
```

### Service Management
```bash
# Enable a service
launchkit enable [service-name]

# Check service health status
launchkit ps
```

## Architecture

### Core Installation Flow
The installation follows a sequence managed by `launchkit`:
1. **init** - Updates system, installs Docker, generates secrets (`lib/system/`, `lib/services/`)
2. **config** - Interactive service selection (`lib/config/wizard.sh`)
3. **up** - Deploys selected services (`lib/services/up.sh`)

### Docker Compose Architecture
- **Profiles**: Services organized into logical groups (n8n, ai-dev, monitoring, langfuse, ollama, speech, etc.)
- **Environment**: All configuration through `.env` files generated during installation
- **Volumes**: Named volumes for data persistence across container rebuilds
- **Networks**: All services on default Docker network with internal service discovery

### Key Service Dependencies
- **n8n**: Requires postgres, redis. Runs in queue mode with configurable worker count
- **Supabase**: Full stack with postgres, auth, storage, analytics, edge functions
- **Langfuse**: Requires postgres, redis, clickhouse, minio for LLM observability
- **Open WebUI**: Integrates with Ollama for local LLMs
- **ComfyUI**: Requires models downloaded separately (FLUX, SDXL, etc.)
- **bolt.diy**: Standalone AI-powered full-stack development platform
- **OpenHands**: AI software engineer with web interface

## Important Implementation Details

### Environment Variable System
- Generated by `03_generate_secrets.sh` using `openssl rand -hex`
- Contains service hostnames, passwords, API keys
- Used by both Docker Compose and Caddy for routing
- AI-specific keys: OPENAI_API_KEY, ANTHROPIC_API_KEY, etc.

### Service Selection Wizard
- `04_wizard.sh` uses whiptail to create interactive checklist
- Updates `COMPOSE_PROFILES` in `.env` based on selections
- Service dependencies automatically handled
- Grouped by category: AI Development, Automation, Databases, Monitoring

### Shared File Access
- `./shared/` directory is mounted to `/data/shared` in containers
- Use this path in n8n workflows and AI tools to share files
- Persistent storage for generated content (images, documents, models)

### AI Service Configuration
- **ComfyUI**: Models stored in `/var/lib/docker/volumes/${PROJECT_NAME:-localai}_comfyui_data/_data/models/`
- **Ollama**: Models persist in `ollama_data` volume
- **Speech Stack**: Uses ports 8001 (Whisper) and 5001 (TTS)
- **Vector DBs**: Qdrant on port 6333, Supabase includes pgvector

### Custom n8n Configuration
- Pre-installs Node.js libraries: cheerio, axios, moment, lodash
- Runs in production mode with PostgreSQL backend
- Queue mode for parallel workflow processing
- Community packages and AI runners enabled
- Configured for tool usage and external function calls
- Metrics enabled for Prometheus monitoring

### Network Architecture
- Caddy handles HTTPS/TLS termination and reverse proxy
- Services exposed via subdomains: n8n.domain.com, comfyui.domain.com, etc.
- Internal services (redis, postgres) not exposed externally
- Supports Cloudflare Tunnel as alternative to port exposure
- sslip.io domains work without DNS configuration

## Common Development Tasks

### Testing Profile Changes
1. Update `COMPOSE_PROFILES` in `.env`
2. Run `launchkit up`
3. Check `launchkit ps` to verify services started

### Adding New AI Services
1. Create a new service directory in `services/` following the structure in `docs/SERVICE_STRUCTURE_SPEC.md`
2. Create `docker-compose.yml` in the service directory
3. Add hostname variables to Caddy environment section (if applicable)
4. Update `config/stacks/core.yaml` (or other stack) to include the service
5. Add Caddyfile routing if service needs web access
6. Consider GPU requirements and volume mounts
7. Document API endpoints and integration points

### Working with AI Models
- ComfyUI models: Download to model directories before use
- Ollama models: Pull with `docker exec ollama ollama pull [model]`
- Embeddings: Configure in n8n AI nodes or Flowise chains
- Vector stores: Initialize indexes in Qdrant or Supabase

### Backup/Restore
- n8n workflows: `./n8n/backup/workflows/`
- ComfyUI workflows: Export from UI
- Vector databases: Use respective backup tools
- All data in Docker volumes: `docker run --rm -v [volume]:/data -v $(pwd):/backup busybox tar czf /backup/[name].tar.gz -C /data .`

### Troubleshooting AI Services
1. Check service logs: `docker compose logs [service-name] -f`
2. Verify GPU access (if applicable): `docker exec [container] nvidia-smi`
3. Monitor resource usage: `docker stats`
4. Check model loading: Service-specific logs
5. API connectivity: Test with curl or service UI

## Security Considerations

- All services require authentication (configured during setup)
- Firewall configured to only allow SSH, HTTP, HTTPS
- Fail2Ban enabled for brute-force protection
- SSL certificates managed by Caddy with Let's Encrypt
- API keys stored in `.env` file (never commit to git)
- Services isolated in Docker network namespaces
- Regular security updates via `update.sh`

## File Structure Notes
- `memory-bank/` - Project documentation and development notes
- `flowise/` - Flowise workflow templates and custom tools
- `n8n/` - n8n configurations and community workflows
- `comfyui/` - ComfyUI custom nodes and workflows (if added)
- `scripts/` - Installation and utility scripts (all bash)
- `shared/` - Shared directory accessible by all containers
- `openedai-config/` - TTS voice configurations
- `openedai-voices/` - Custom TTS voice models

## AI LaunchKit Specific Features

### Speech Stack Integration
- **Whisper (STT)**: OpenAI-compatible API on port 8001
- **OpenedAI (TTS)**: Multiple voices, OpenAI-compatible API on port 5001
- Both services integrate seamlessly with n8n HTTP nodes

### Development Platforms
- **bolt.diy**: Full-stack development with AI assistance
- **OpenHands**: Autonomous coding agent with web interface
- **OpenUI**: AI-powered UI component generator

### Planned Additions
- **LiveKit**: Real-time communication and AI agents
- **browserless**: Headless Chrome for web automation
- **PaddleOCR**: Document OCR processing

## Critical Implementation Notes

- Never modify the installation script sequence (01-06) without understanding dependencies
- Always use logging functions from `utils.sh` in new scripts
- The `.env` file contains all secrets and must never be committed
- Services use Docker health checks - respect dependency conditions
- Profile-based deployment allows selective service activation
- AI services may require significant resources (RAM, disk, GPU)
- Community workflows are imported during installation (20-30 minutes)
- Model downloads for ComfyUI/Ollama happen post-installation
- Test AI integrations with small workloads before scaling

## Contributing Guidelines

When contributing new AI services or features:
1. Follow existing patterns in docker-compose.yml
2. Add comprehensive health checks
3. Document resource requirements
4. Include example workflows or usage
5. Test with minimal and full installations
6. Update this CLAUDE.md file with relevant information

## Useful Resources
- Repository: https://github.com/freddy-schuetz/ai-launchkit
- Based on: n8n-installer by @kossakovsky
- License: Apache 2.0 (commercial use allowed)
